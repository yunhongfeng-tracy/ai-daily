#!/usr/bin/env python3
"""
AI Daily Generator - è‡ªåŠ¨ç”Ÿæˆæ¯æ—¥AIæ–°é—»å’Œå·¥å…·æ¨è
"""

import os
import re
import json
import subprocess
import time
import urllib.request
import urllib.parse
from datetime import datetime
from urllib.parse import urlparse

# é…ç½®
REPO_DIR = "/root/.openclaw/workspace/ai-daily"
TODAY = datetime.now().strftime('%Y-%m-%d')
NOW = datetime.now().strftime('%Y-%m-%d %H:%M')
def _load_env_from_secrets():
    p = "/root/.openclaw/workspace/.secrets/credentials.env"
    try:
        with open(p, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("#") or not line.startswith("export "):
                    continue
                k, v = line[len("export "):].split("=", 1)
                v = v.strip().strip("'\"")
                os.environ.setdefault(k.strip(), v)
    except Exception:
        pass

_load_env_from_secrets()

DEEPSEEK_API_KEY = os.environ.get('DEEPSEEK_API_KEY')
GITHUB_TOKEN = os.environ.get('GITHUB_TOKEN')
if not DEEPSEEK_API_KEY:
    print("âš ï¸ DEEPSEEK_API_KEY not set; translations may fail")


# DeepSeekç¿»è¯‘å‡½æ•°
def translate_with_deepseek(text):
    """ä½¿ç”¨DeepSeek APIç¿»è¯‘ä¸ºä¸­æ–‡"""
    if not text or len(text.strip()) < 5:
        return text
    
    # ç®€å•æœ¯è¯­ç›´æ¥æŸ¥è¯å…¸ï¼ˆå¿«é€Ÿï¼‰
    simple_trans = {
        'AI': 'äººå·¥æ™ºèƒ½',
        'Artificial Intelligence': 'äººå·¥æ™ºèƒ½',
        'Machine Learning': 'æœºå™¨å­¦ä¹ ',
        'Deep Learning': 'æ·±åº¦å­¦ä¹ ',
        'LLM': 'å¤§è¯­è¨€æ¨¡å‹',
        'OpenAI': 'OpenAI',
        'Anthropic': 'Anthropic',
        'Google': 'è°·æ­Œ',
        'Microsoft': 'å¾®è½¯',
        'Reuters': 'è·¯é€ç¤¾',
        'BBC': 'BBC',
        'MIT': 'éº»çœç†å·¥',
        'TechCrunch': 'TechCrunch',
        'NVIDIA': 'è‹±ä¼Ÿè¾¾',
        'Meta': 'Meta',
        'Amazon': 'äºšé©¬é€Š',
        'Apple': 'è‹¹æœ',
    }
    
    # å…ˆåšç®€å•æ›¿æ¢
    result = text
    for eng, chi in simple_trans.items():
        result = re.sub(r'\b' + re.escape(eng) + r'\b', chi, result, flags=re.IGNORECASE)
    
    # å¦‚æœåŒ…å«å¤æ‚å¥å­ï¼Œç”¨DeepSeekç¿»è¯‘
    if len(text) > 30 and not text.startswith('http'):
        try:
            url = "https://api.deepseek.com/chat/completions"
            payload = {
                "model": "deepseek-chat",
                "messages": [
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIç§‘æŠ€æ–°é—»ç¿»è¯‘ã€‚è¯·å°†è‹±æ–‡ç¿»è¯‘æˆç®€æ´çš„ä¸­æ–‡ï¼Œä¿ç•™ä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§ã€‚åªéœ€è¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"
                    },
                    {
                        "role": "user",
                        "content": f"ç¿»è¯‘è¿™æ®µè‹±æ–‡æ–°é—»æ ‡é¢˜å’Œæ‘˜è¦ï¼š\n\n{text}"
                    }
                ],
                "max_tokens": 500,
                "temperature": 0.3
            }
            
            data = json.dumps(payload).encode('utf-8')
            req = urllib.request.Request(url, data=data, method='POST')
            req.add_header('Content-Type', 'application/json')
            req.add_header('Authorization', f'Bearer {DEEPSEEK_API_KEY}')
            
            with urllib.request.urlopen(req, timeout=30) as response:
                result_data = json.loads(response.read().decode('utf-8'))
                translated = result_data['choices'][0]['message']['content'].strip()
                # æ¸…ç†å¯èƒ½çš„å¼•å·
                translated = re.sub(r'^["\']|["\']$', '', translated)
                return translated
        except Exception as e:
            print(f"  ç¿»è¯‘APIè°ƒç”¨å¤±è´¥: {e}")
            return result
    
    return result

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬"""
    if not text:
        return ''
    # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    # æ›¿æ¢HTMLå®ä½“
    text = text.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
    text = text.replace('&#x27;', "'").replace('&#39;', "'").replace('&quot;', '"')
    text = text.replace('&ldquo;', '"').replace('&rdquo;', '"').replace('&lsquo;', "'").replace('&rsquo;', "'")
    # ç§»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
    text = re.sub(r'\s+', ' ', text).strip()
    # ç§»é™¤æ¨å¹¿ä¿¡æ¯
    text = re.sub(r'Subscribe.*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'Register.*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'Login.*', '', text, flags=re.IGNORECASE)
    return text.strip()

def get_source_name(url):
    """ä»URLæå–æ¥æº"""
    if not url:
        return 'æœªçŸ¥æ¥æº'
    try:
        parsed = urlparse(url)
        source = parsed.netloc.replace('www.', '').split('/')[0]
        # å¸¸è§æ¥æºæ˜ å°„
        source_map = {
            'reuters.com': 'è·¯é€ç¤¾',
            'bbc.com': 'BBC',
            'techcrunch.com': 'TechCrunch',
            'mit.edu': 'éº»çœç†å·¥',
            'theverge.com': 'The Verge',
            'wired.com': 'Wired',
            'artificialintelligence-news.com': 'AIæ–°é—»',
        }
        for eng, chi in source_map.items():
            if eng in source:
                return chi
        return source.split('.')[0].title()
    except:
        return 'æœªçŸ¥æ¥æº'

def _parse_iso_dt(s: str):
    if not s:
        return None
    try:
        # Brave returns ISO timestamps like 2026-02-20T05:39:45
        return datetime.fromisoformat(s)
    except Exception:
        return None


def _is_reputable_source(url: str) -> bool:
    """Very conservative allowlist for 'æœ‰æ•ˆæ–°é—»' quality."""
    if not url:
        return False
    host = urlparse(url).netloc.lower().replace("www.", "")

    # hard deny
    if host.endswith("wikipedia.org"):
        return False

    allow = {
        # mainstream tech/business
        "reuters.com",
        "bloomberg.com",
        "ft.com",
        "wsj.com",
        "cnbc.com",
        "axios.com",
        "theverge.com",
        "arstechnica.com",
        "wired.com",
        "techcrunch.com",
        "venturebeat.com",
        "spectrum.ieee.org",
        "sfchronicle.com",
        "pcmag.com",
        "bbc.com",
        "theguardian.com",
        "nytimes.com",
        "washingtonpost.com",
        "economist.com",
        "forbes.com",
        # science
        "nature.com",
        "science.org",
        "mit.edu",
        # vendor / labs / official
        "openai.com",
        "anthropic.com",
        "deepmind.google",
        "blog.google",
        "ai.google.dev",
        "cloud.google.com",
        "microsoft.com",
        "nvidia.com",
        "huggingface.co",
        # ecosystem
        "github.com",
    }

    return host in allow


def _is_probable_homepage_or_section(url: str, title: str) -> bool:
    """Reject non-article pages (homepages/sections/category indexes)."""
    try:
        p = urlparse(url)
        host = p.netloc.lower().replace("www.", "")
        path = (p.path or "/").rstrip("/")
        t = (title or "").lower()

        if "wikipedia" in host or "wikipedia" in t:
            return True

        # obvious home/section pages
        if path in {"", "/", "/technology", "/tech", "/ai"}:
            return True

        # Reuters section pages are common results; avoid them.
        if host == "reuters.com" and path in {"/technology", "/world", "/business"}:
            return True

        # generic â€œnews hubâ€ titles
        if re.search(r"\b(latest|today|news)\b", t) and ("/" not in (p.path or "").strip("/")):
            return True

        return False
    except Exception:
        return False


def _looks_like_real_news_item(title: str, desc: str) -> bool:
    t = (title or "").lower()
    d = (desc or "").lower()

    # avoid homepages/aggregators/SEO sludge
    bad_patterns = [
        r"\blatest news\b",
        r"\bai news\b\s*\|",
        r"\bhome\b",
        r"\bnewsletter\b",
        r"\bsubscribe\b",
        r"\bregister\b",
        r"\blogin\b",
        r"\bpricing\b",
        r"\bjobs\b",
    ]
    if any(re.search(p, t) for p in bad_patterns):
        return False

    # require at least some "event" signal
    signal_words = [
        "launch",
        "released",
        "release",
        "announces",
        "announced",
        "unveils",
        "debut",
        "funding",
        "raises",
        "acquires",
        "acquisition",
        "partnership",
        "regulation",
        "lawsuit",
        "ban",
        "policy",
        "model",
        "chip",
        "gpu",
        "security",
        "openai",
        "anthropic",
        "google",
        "microsoft",
        "nvidia",
        "deepseek",
        "qwen",
        "gemini",
        "claude",
    ]
    blob = f"{t} {d}"
    return any(w in blob for w in signal_words)


def _score_item(item: dict) -> float:
    """Cheap heuristic score: prioritize recency + reputable domains."""
    url = item.get("url", "")
    host = (item.get("meta_url") or {}).get("netloc", "")
    host = (host or urlparse(url).netloc).lower().replace("www.", "")

    # domain weights
    domain_boost = 0.0
    if host in {"reuters.com", "bloomberg.com", "ft.com", "wsj.com"}:
        domain_boost = 3.0
    elif host in {"theverge.com", "arstechnica.com", "wired.com", "techcrunch.com", "axios.com", "cnbc.com"}:
        domain_boost = 2.0
    elif host in {"openai.com", "anthropic.com", "ai.google.dev", "cloud.google.com", "microsoft.com", "nvidia.com"}:
        domain_boost = 2.5
    elif host:
        domain_boost = 0.5

    # recency: newer => higher
    page_age = _parse_iso_dt(item.get("page_age"))
    recency = 0.0
    if page_age:
        hours = (datetime.now() - page_age).total_seconds() / 3600
        # clamp (0..72h)
        hours = max(0.0, min(72.0, hours))
        recency = (72.0 - hours) / 24.0  # 0..3

    return domain_boost + recency


def search_news():
    """æœç´¢AIæ–°é—»ï¼ˆå¹¶åšç­›é€‰ï¼šè¿‘ä¸¤å¤© + å¯ä¿¡æ¥æº + æ›´åƒæ–°é—»çš„æ¡ç›®ï¼‰"""
    print(f"ğŸ¤– AI Daily Generator - {TODAY}")
    print("ğŸ“° æœç´¢AIæ–°é—»...")

    BRAVE_API_KEY = os.environ.get('BRAVE_API_KEY')
    if not BRAVE_API_KEY:
        print("æœç´¢å¤±è´¥: BRAVE_API_KEY not set")
        return None

    # Freshness: past day; we do 2 queries (rate-limited to 1 QPS on free plan).
    queries = [
        "OpenAI Anthropic Google Microsoft NVIDIA DeepSeek Qwen Gemini Claude AI news",
        "AI model released benchmark safety regulation funding NVIDIA chip",
    ]

    merged_results = []
    data = {"web": {"results": merged_results}}

    try:
        for idx, q in enumerate(queries):
            params = {"q": q, "count": 20, "freshness": "pd"}
            url = "https://api.search.brave.com/res/v1/web/search?" + urllib.parse.urlencode(params)
            req = urllib.request.Request(url, headers={
                'Accept': 'application/json',
                'X-Subscription-Token': BRAVE_API_KEY
            })

            # avoid 429 on Free plan (1 QPS)
            if idx > 0:
                time.sleep(1.2)

            with urllib.request.urlopen(req, timeout=30) as response:
                chunk = json.loads(response.read().decode('utf-8'))
                merged_results.extend(((chunk.get('web', {}) or {}).get('results', [])) or [])

        # Filter + rank in-place so the rest of the pipeline stays simple.
        results = merged_results
        filtered = []
        seen = set()

        cutoff_hours = 48
        now = datetime.now()

        def recency_ok(item_):
            page_age_ = _parse_iso_dt(item_.get('page_age'))
            if not page_age_:
                return True  # keep unknown, but will be scored lower
            age_hours_ = (now - page_age_).total_seconds() / 3600
            return age_hours_ <= cutoff_hours

        def add_item(item_):
            title_ = clean_text(item_.get('title', ''))
            url_ = item_.get('url', '')
            if not title_ or not url_:
                return False
            key_ = (re.sub(r"\W+", "", title_.lower())[:80], urlparse(url_).netloc.lower())
            if key_ in seen:
                return False
            seen.add(key_)
            filtered.append(item_)
            return True

        # Pass 1: strict (reputable + looks like news + recency)
        for item in results:
            title = clean_text(item.get('title', ''))
            url_i = item.get('url', '')
            desc = clean_text(item.get('description', ''))

            if not title or not url_i:
                continue
            if not recency_ok(item):
                continue
            if _is_probable_homepage_or_section(url_i, title):
                continue
            if not _is_reputable_source(url_i):
                continue
            if not _looks_like_real_news_item(title, desc):
                continue
            add_item(item)

        # Pass 2: relax "news signal" if we have too few
        if len(filtered) < 5:
            for item in results:
                title = clean_text(item.get('title', ''))
                url_i = item.get('url', '')
                if not title or not url_i:
                    continue
                if not recency_ok(item):
                    continue
                if _is_probable_homepage_or_section(url_i, title):
                    continue
                if not _is_reputable_source(url_i):
                    continue
                add_item(item)
                if len(filtered) >= 7:
                    break

        # Pass 3: last resort â€” still require reputable sources, only relax the "news signal".
        if len(filtered) < 5:
            for item in results:
                title = clean_text(item.get('title', ''))
                url_i = item.get('url', '')
                if not title or not url_i:
                    continue
                if not recency_ok(item):
                    continue
                if _is_probable_homepage_or_section(url_i, title):
                    continue
                if not _is_reputable_source(url_i):
                    continue
                add_item(item)
                if len(filtered) >= 7:
                    break

        filtered.sort(key=_score_item, reverse=True)
        data.setdefault('web', {})['results'] = filtered
        print(f"âœ“ åŸå§‹ç»“æœ {len(results)} æ¡ï¼Œç­›é€‰å {len(filtered)} æ¡")
        return data

    except Exception as e:
        # Try to print response body for HTTPError (useful for 422 debugging)
        try:
            from urllib import error as urllib_error
            if isinstance(e, urllib_error.HTTPError):
                body = e.read().decode('utf-8', errors='ignore')
                print(f"æœç´¢å¤±è´¥: HTTP {e.code} {e.reason}; body: {body[:300]}")
                return None
        except Exception:
            pass
        print(f"æœç´¢å¤±è´¥: {e}")
        return None

def generate_daily():
    """ç”Ÿæˆæ—¥æŠ¥"""
    data = search_news()
    
    md_file = os.path.join(REPO_DIR, 'daily', f'{TODAY}.md')
    
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(f"# AI Daily Â· {TODAY}\n\n")
        f.write(f"æ—¥æœŸ: {TODAY} {datetime.now().strftime('%H:%M')}\n\n")
        
        # ä»Šæ—¥æ–°é—»
        f.write("## ğŸ“° ä»Šæ—¥æ–°é—»\n\n")
        
        if data and 'web' in data:
            for item in data.get('web', {}).get('results', [])[:5]:
                title = clean_text(item.get('title', ''))
                url = item.get('url', '')
                desc = clean_text(item.get('description', ''))
                
                if title and url:
                    # ä½¿ç”¨DeepSeekç¿»è¯‘æ ‡é¢˜
                    title_cn = translate_with_deepseek(title)
                    
                    source = get_source_name(url)
                    
                    f.write(f"### {title_cn}\n\n")
                    f.write(f"æ¥æº: [{source}]({url})\n\n")
                    if desc:
                        # ä½¿ç”¨DeepSeekç¿»è¯‘æè¿°
                        desc_cn = translate_with_deepseek(desc)
                        f.write(f"{desc_cn}\n\n")
                    f.write(f"[é˜…è¯»åŸæ–‡]({url})\n\n")
                    f.write("---\n\n")
        
        # å·¥å…·æ¨è
        f.write("## ğŸ› ï¸ å·¥å…·æ¨è\n\n")
        tools = [
            ("v0.dev - AI UIç”Ÿæˆå™¨", "ç”±Vercelæ¨å‡ºçš„AIç•Œé¢ç”Ÿæˆå™¨ï¼Œåªéœ€æè¿°éœ€æ±‚å³å¯è‡ªåŠ¨ç”ŸæˆReact/Tailwindç»„ä»¶ã€‚", "https://v0.app"),
            ("Cursor - AIä»£ç ç¼–è¾‘å™¨", "ä¸“ä¸ºAIè¾…åŠ©ç¼–ç¨‹è®¾è®¡çš„IDEï¼ŒåŸºäºVS Codeï¼Œæ”¯æŒæ™ºèƒ½ä»£ç è¡¥å…¨å’Œé‡æ„å»ºè®®ã€‚", "https://cursor.com"),
            ("Perplexity - AIæœç´¢å¼•æ“", "ç»“åˆå¤§è¯­è¨€æ¨¡å‹çš„æœç´¢å¼•æ“ï¼Œæä¾›å¸¦æœ‰å¼•ç”¨æ¥æºçš„ç­”æ¡ˆï¼Œæ”¯æŒå¤šç§è¯­è¨€ã€‚", "https://www.perplexity.ai"),
        ]
        
        for name, desc, link in tools:
            f.write(f"### {name}\n\n")
            f.write(f"ğŸ“ {desc}\n\n")
            f.write(f"ğŸ”— [è®¿é—®]({link})\n\n")
            f.write("---\n\n")
        
        # å½’æ¡£
        f.write("## ğŸ“š å½’æ¡£\n")
        f.write(f"- [{TODAY}](./{TODAY}.html)\n")
    
    print(f"âœ“ åˆ›å»ºæ—¥æŠ¥: {md_file}")
    
    # æ›´æ–°README
    readme_file = os.path.join(REPO_DIR, 'README.md')
    if os.path.exists(readme_file):
        with open(readme_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ç§»é™¤æ—§æ¡ç›®ï¼Œæ·»åŠ æ–°æ¡ç›®
        content = re.sub(r'- \[{}\].*\n'.format(TODAY), '', content)
        content = re.sub(r'(\n## ğŸ“š å½’æ¡£)', f'\n- [{TODAY}](./daily/{TODAY}.md)\n\\1', content)
        
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write(content)
        print("âœ“ æ›´æ–°README")
    
    return md_file

def generate_html():
    """ç”ŸæˆHTML"""
    print("ğŸ”„ ç”ŸæˆHTMLé¡µé¢...")
    convert_script = os.path.join(REPO_DIR, 'convert.py')
    result = subprocess.run(['python3', convert_script], capture_output=True, text=True)
    if result.returncode == 0:
        print("âœ“ ç”ŸæˆHTMLé¡µé¢")
    else:
        print(f"âœ— HTMLç”Ÿæˆå¤±è´¥: {result.stderr}")

def commit_and_push():
    """æäº¤å¹¶æ¨é€"""
    print("ğŸ“¤ æ¨é€åˆ°GitHub...")
    
    # è®¾ç½®remote
    if GITHUB_TOKEN:
        remote_url = f"https://{GITHUB_TOKEN}@github.com/yunhongfeng-tracy/ai-daily.git"
        subprocess.run(['git', 'remote', 'set-url', 'origin', remote_url], cwd=REPO_DIR, capture_output=True)
    
    # é…ç½®git
    subprocess.run(['git', 'config', 'user.name', 'tracy-bot'], cwd=REPO_DIR, capture_output=True)
    subprocess.run(['git', 'config', 'user.email', 'bot@tracy.ai'], cwd=REPO_DIR, capture_output=True)
    
    # æ·»åŠ å¹¶æäº¤
    subprocess.run(['git', 'add', '-A'], cwd=REPO_DIR, capture_output=True)
    result = subprocess.run(['git', 'status', '--porcelain'], cwd=REPO_DIR, capture_output=True, text=True)
    
    if result.stdout.strip():
        subprocess.run(['git', 'commit', '-m', f'AI Daily: {TODAY}'], cwd=REPO_DIR, capture_output=True)
        if GITHUB_TOKEN:
            subprocess.run(['git', 'push', 'origin', 'main'], cwd=REPO_DIR, capture_output=True)
            print("âœ“ å·²æ¨é€åˆ°GitHub")
    else:
        print("âœ“ æ— æ–°å†…å®¹ï¼Œè·³è¿‡æäº¤")

def main():
    print("=" * 40)
    md_file = generate_daily()
    generate_html()
    commit_and_push()
    print("=" * 40)
    print(f"ğŸ‰ AIæ—¥æŠ¥ç”Ÿæˆå®Œæˆï¼")
    print(f"ğŸ“… æ—¥æœŸ: {TODAY}")

if __name__ == '__main__':
    main()
