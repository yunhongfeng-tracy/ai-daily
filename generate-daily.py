#!/usr/bin/env python3
"""
AI Daily Generator - è‡ªåŠ¨ç”Ÿæˆæ¯æ—¥AIæ–°é—»å’Œå·¥å…·æ¨è
"""

import os
import re
import json
import subprocess
import time
import urllib.request
import urllib.parse
from datetime import datetime
from urllib.parse import urlparse

# é…ç½®
REPO_DIR = "/root/.openclaw/workspace/ai-daily"
TODAY = datetime.now().strftime('%Y-%m-%d')
NOW = datetime.now().strftime('%Y-%m-%d %H:%M')
def _load_env_from_secrets():
    p = "/root/.openclaw/workspace/.secrets/credentials.env"
    try:
        with open(p, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("#") or not line.startswith("export "):
                    continue
                k, v = line[len("export "):].split("=", 1)
                v = v.strip().strip("'\"")
                os.environ.setdefault(k.strip(), v)
    except Exception:
        pass

_load_env_from_secrets()

DEEPSEEK_API_KEY = os.environ.get('DEEPSEEK_API_KEY')
GITHUB_TOKEN = os.environ.get('GITHUB_TOKEN')
if not DEEPSEEK_API_KEY:
    print("âš ï¸ DEEPSEEK_API_KEY not set; translations may fail")


# DeepSeekç¿»è¯‘å‡½æ•°
def translate_with_deepseek(text):
    """ä½¿ç”¨DeepSeek APIç¿»è¯‘ä¸ºä¸­æ–‡"""
    if not text or len(text.strip()) < 5:
        return text
    
    # ç®€å•æœ¯è¯­ç›´æ¥æŸ¥è¯å…¸ï¼ˆå¿«é€Ÿï¼‰
    simple_trans = {
        'AI': 'äººå·¥æ™ºèƒ½',
        'Artificial Intelligence': 'äººå·¥æ™ºèƒ½',
        'Machine Learning': 'æœºå™¨å­¦ä¹ ',
        'Deep Learning': 'æ·±åº¦å­¦ä¹ ',
        'LLM': 'å¤§è¯­è¨€æ¨¡å‹',
        'OpenAI': 'OpenAI',
        'Anthropic': 'Anthropic',
        'Google': 'è°·æ­Œ',
        'Microsoft': 'å¾®è½¯',
        'Reuters': 'è·¯é€ç¤¾',
        'BBC': 'BBC',
        'MIT': 'éº»çœç†å·¥',
        'TechCrunch': 'TechCrunch',
        'NVIDIA': 'è‹±ä¼Ÿè¾¾',
        'Meta': 'Meta',
        'Amazon': 'äºšé©¬é€Š',
        'Apple': 'è‹¹æœ',
    }
    
    # å…ˆåšç®€å•æ›¿æ¢
    result = text
    for eng, chi in simple_trans.items():
        result = re.sub(r'\b' + re.escape(eng) + r'\b', chi, result, flags=re.IGNORECASE)
    
    # å¦‚æœåŒ…å«å¤æ‚å¥å­ï¼Œç”¨DeepSeekç¿»è¯‘
    if len(text) > 30 and not text.startswith('http'):
        try:
            url = "https://api.deepseek.com/chat/completions"
            payload = {
                "model": "deepseek-chat",
                "messages": [
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIç§‘æŠ€æ–°é—»ç¿»è¯‘ã€‚è¯·å°†è‹±æ–‡ç¿»è¯‘æˆç®€æ´çš„ä¸­æ–‡ï¼Œä¿ç•™ä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§ã€‚åªéœ€è¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"
                    },
                    {
                        "role": "user",
                        "content": f"ç¿»è¯‘è¿™æ®µè‹±æ–‡æ–°é—»æ ‡é¢˜å’Œæ‘˜è¦ï¼š\n\n{text}"
                    }
                ],
                "max_tokens": 500,
                "temperature": 0.3
            }
            
            data = json.dumps(payload).encode('utf-8')
            req = urllib.request.Request(url, data=data, method='POST')
            req.add_header('Content-Type', 'application/json')
            req.add_header('Authorization', f'Bearer {DEEPSEEK_API_KEY}')
            
            with urllib.request.urlopen(req, timeout=30) as response:
                result_data = json.loads(response.read().decode('utf-8'))
                translated = result_data['choices'][0]['message']['content'].strip()
                # æ¸…ç†å¯èƒ½çš„å¼•å·
                translated = re.sub(r'^["\']|["\']$', '', translated)
                return translated
        except Exception as e:
            print(f"  ç¿»è¯‘APIè°ƒç”¨å¤±è´¥: {e}")
            return result
    
    return result

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬"""
    if not text:
        return ''
    # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    # æ›¿æ¢HTMLå®ä½“
    text = text.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
    text = text.replace('&#x27;', "'").replace('&#39;', "'").replace('&quot;', '"')
    text = text.replace('&ldquo;', '"').replace('&rdquo;', '"').replace('&lsquo;', "'").replace('&rsquo;', "'")
    # ç§»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
    text = re.sub(r'\s+', ' ', text).strip()
    # ç§»é™¤æ¨å¹¿ä¿¡æ¯
    text = re.sub(r'Subscribe.*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'Register.*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'Login.*', '', text, flags=re.IGNORECASE)
    return text.strip()

def get_source_name(url):
    """ä»URLæå–æ¥æº"""
    if not url:
        return 'æœªçŸ¥æ¥æº'
    try:
        parsed = urlparse(url)
        source = parsed.netloc.replace('www.', '').split('/')[0]
        # å¸¸è§æ¥æºæ˜ å°„
        source_map = {
            'reuters.com': 'è·¯é€ç¤¾',
            'bbc.com': 'BBC',
            'techcrunch.com': 'TechCrunch',
            'mit.edu': 'éº»çœç†å·¥',
            'theverge.com': 'The Verge',
            'wired.com': 'Wired',
            'artificialintelligence-news.com': 'AIæ–°é—»',
        }
        for eng, chi in source_map.items():
            if eng in source:
                return chi
        return source.split('.')[0].title()
    except:
        return 'æœªçŸ¥æ¥æº'

def _parse_iso_dt(s: str):
    if not s:
        return None
    try:
        # Brave returns ISO timestamps like 2026-02-20T05:39:45
        return datetime.fromisoformat(s)
    except Exception:
        return None


def _is_reputable_source(url: str) -> bool:
    """Very conservative allowlist for 'æœ‰æ•ˆæ–°é—»' quality."""
    if not url:
        return False
    host = urlparse(url).netloc.lower().replace("www.", "")

    # hard deny
    if host.endswith("wikipedia.org"):
        return False

    allow = {
        # mainstream tech/business
        "reuters.com",
        "bloomberg.com",
        "ft.com",
        "wsj.com",
        "cnbc.com",
        "axios.com",
        "theverge.com",
        "arstechnica.com",
        "wired.com",
        "techcrunch.com",
        "venturebeat.com",
        "spectrum.ieee.org",
        "sfchronicle.com",
        "pcmag.com",
        "bbc.com",
        "theguardian.com",
        "nytimes.com",
        "washingtonpost.com",
        "economist.com",
        "forbes.com",
        "zdnet.com",
        "tomshardware.com",
        "semafor.com",
        "theinformation.com",
        # science
        "nature.com",
        "science.org",
        "mit.edu",
        # vendor / labs / official
        "openai.com",
        "anthropic.com",
        "deepmind.google",
        "blog.google",
        "ai.google.dev",
        "cloud.google.com",
        "microsoft.com",
        "nvidia.com",
        "huggingface.co",
        # ecosystem
        "github.com",
    }

    return host in allow


def _is_probable_homepage_or_section(url: str, title: str) -> bool:
    """Reject non-article pages (homepages/sections/category indexes)."""
    try:
        p = urlparse(url)
        host = p.netloc.lower().replace("www.", "")
        path = (p.path or "/").rstrip("/")
        t = (title or "").lower()

        if "wikipedia" in host or "wikipedia" in t:
            return True

        # obvious home/section pages
        if path in {"", "/", "/technology", "/tech", "/ai"}:
            return True

        # Reuters section pages are common results; avoid them.
        if host == "reuters.com" and path in {"/technology", "/world", "/business"}:
            return True

        # Product Hunt category/review/alternative pages are not tool launches
        if host == "producthunt.com":
            bad_prefixes = ("/categories/", "/products/")
            if path.startswith(bad_prefixes) and (
                "/reviews" in path or "/alternatives" in path or path.startswith("/categories/")
            ):
                return True

        # generic â€œnews hubâ€ titles
        if re.search(r"\b(latest|today|news)\b", t) and ("/" not in (p.path or "").strip("/")):
            return True

        return False
    except Exception:
        return False


def _looks_like_real_news_item(title: str, desc: str) -> bool:
    t = (title or "").lower()
    d = (desc or "").lower()

    # avoid homepages/aggregators/SEO sludge
    bad_patterns = [
        r"\blatest news\b",
        r"\bai news\b\s*\|",
        r"\bhome\b",
        r"\bnewsletter\b",
        r"\bsubscribe\b",
        r"\bregister\b",
        r"\blogin\b",
        r"\bpricing\b",
        r"\bjobs\b",
    ]
    if any(re.search(p, t) for p in bad_patterns):
        return False

    # require at least some "event" signal
    signal_words = [
        "launch",
        "released",
        "release",
        "announces",
        "announced",
        "unveils",
        "debut",
        "funding",
        "raises",
        "acquires",
        "acquisition",
        "partnership",
        "regulation",
        "lawsuit",
        "ban",
        "policy",
        "model",
        "chip",
        "gpu",
        "security",
        "openai",
        "anthropic",
        "google",
        "microsoft",
        "nvidia",
        "deepseek",
        "qwen",
        "gemini",
        "claude",
    ]
    blob = f"{t} {d}"
    return any(w in blob for w in signal_words)


def _score_item(item: dict) -> float:
    """Cheap heuristic score: prioritize recency + reputable domains."""
    url = item.get("url", "")
    host = (item.get("meta_url") or {}).get("netloc", "")
    host = (host or urlparse(url).netloc).lower().replace("www.", "")

    # domain weights
    domain_boost = 0.0
    if host in {"reuters.com", "bloomberg.com", "ft.com", "wsj.com"}:
        domain_boost = 3.0
    elif host in {"theverge.com", "arstechnica.com", "wired.com", "techcrunch.com", "axios.com", "cnbc.com"}:
        domain_boost = 2.0
    elif host in {"openai.com", "anthropic.com", "ai.google.dev", "cloud.google.com", "microsoft.com", "nvidia.com"}:
        domain_boost = 2.5
    elif host:
        domain_boost = 0.5

    # recency: newer => higher
    page_age = _parse_iso_dt(item.get("page_age"))
    recency = 0.0
    if page_age:
        hours = (datetime.now() - page_age).total_seconds() / 3600
        # clamp (0..72h)
        hours = max(0.0, min(72.0, hours))
        recency = (72.0 - hours) / 24.0  # 0..3

    return domain_boost + recency


def search_news():
    """æœç´¢AIæ–°é—»ï¼ˆå¹¶åšç­›é€‰ï¼šè¿‘ä¸¤å¤© + å¯ä¿¡æ¥æº + æ›´åƒæ–°é—»çš„æ¡ç›®ï¼‰"""
    print(f"ğŸ¤– AI Daily Generator - {TODAY}")
    print("ğŸ“° æœç´¢AIæ–°é—»...")

    BRAVE_API_KEY = os.environ.get('BRAVE_API_KEY')
    if not BRAVE_API_KEY:
        print("æœç´¢å¤±è´¥: BRAVE_API_KEY not set")
        return None

    # Freshness: past day; we do 2 queries (rate-limited to 1 QPS on free plan).
    queries = [
        "OpenAI Anthropic Google Microsoft NVIDIA DeepSeek Qwen Gemini Claude AI news",
        "AI model released benchmark safety regulation funding NVIDIA chip",
        "EU US AI regulation policy lawsuit copyright OpenAI Anthropic",
    ]

    merged_results = []
    data = {"web": {"results": merged_results}}

    try:
        for idx, q in enumerate(queries):
            params = {"q": q, "count": 20, "freshness": "pd"}
            url = "https://api.search.brave.com/res/v1/web/search?" + urllib.parse.urlencode(params)
            req = urllib.request.Request(url, headers={
                'Accept': 'application/json',
                'X-Subscription-Token': BRAVE_API_KEY
            })

            # avoid 429 on Free plan (1 QPS)
            if idx > 0:
                time.sleep(1.2)

            with urllib.request.urlopen(req, timeout=30) as response:
                chunk = json.loads(response.read().decode('utf-8'))
                merged_results.extend(((chunk.get('web', {}) or {}).get('results', [])) or [])

        # Filter + rank in-place so the rest of the pipeline stays simple.
        results = merged_results
        filtered = []
        seen = set()

        cutoff_hours = 72
        now = datetime.now()

        def recency_ok(item_):
            page_age_ = _parse_iso_dt(item_.get('page_age'))
            if not page_age_:
                return True  # keep unknown, but will be scored lower
            age_hours_ = (now - page_age_).total_seconds() / 3600
            return age_hours_ <= cutoff_hours

        def add_item(item_):
            title_ = clean_text(item_.get('title', ''))
            url_ = item_.get('url', '')
            if not title_ or not url_:
                return False
            key_ = (re.sub(r"\W+", "", title_.lower())[:80], urlparse(url_).netloc.lower())
            if key_ in seen:
                return False
            seen.add(key_)
            filtered.append(item_)
            return True

        # Pass 1: strict (reputable + looks like news + recency)
        for item in results:
            title = clean_text(item.get('title', ''))
            url_i = item.get('url', '')
            desc = clean_text(item.get('description', ''))

            if not title or not url_i:
                continue
            if not recency_ok(item):
                continue
            if _is_probable_homepage_or_section(url_i, title):
                continue
            if not _is_reputable_source(url_i):
                continue
            if not _looks_like_real_news_item(title, desc):
                continue
            add_item(item)

        # Pass 2: relax "news signal" if we have too few
        if len(filtered) < 5:
            for item in results:
                title = clean_text(item.get('title', ''))
                url_i = item.get('url', '')
                if not title or not url_i:
                    continue
                if not recency_ok(item):
                    continue
                if _is_probable_homepage_or_section(url_i, title):
                    continue
                if not _is_reputable_source(url_i):
                    continue
                add_item(item)
                if len(filtered) >= 7:
                    break

        # Pass 3: last resort â€” still require reputable sources, only relax the "news signal".
        if len(filtered) < 5:
            for item in results:
                title = clean_text(item.get('title', ''))
                url_i = item.get('url', '')
                if not title or not url_i:
                    continue
                if not recency_ok(item):
                    continue
                if _is_probable_homepage_or_section(url_i, title):
                    continue
                if not _is_reputable_source(url_i):
                    continue
                add_item(item)
                if len(filtered) >= 7:
                    break

        # Pass 4: if still too few, broaden recency to 7 days (still reputable + not homepage)
        if len(filtered) < 5:
            broaden_hours = 168
            for item in results:
                title = clean_text(item.get('title', ''))
                url_i = item.get('url', '')
                if not title or not url_i:
                    continue
                page_age = _parse_iso_dt(item.get('page_age'))
                if page_age:
                    age_hours = (now - page_age).total_seconds() / 3600
                    if age_hours > broaden_hours:
                        continue
                if _is_probable_homepage_or_section(url_i, title):
                    continue
                if not _is_reputable_source(url_i):
                    continue
                add_item(item)
                if len(filtered) >= 6:
                    break

        filtered.sort(key=_score_item, reverse=True)
        data.setdefault('web', {})['results'] = filtered
        print(f"âœ“ åŸå§‹ç»“æœ {len(results)} æ¡ï¼Œç­›é€‰å {len(filtered)} æ¡")
        return data

    except Exception as e:
        # Try to print response body for HTTPError (useful for 422 debugging)
        try:
            from urllib import error as urllib_error
            if isinstance(e, urllib_error.HTTPError):
                body = e.read().decode('utf-8', errors='ignore')
                print(f"æœç´¢å¤±è´¥: HTTP {e.code} {e.reason}; body: {body[:300]}")
                return None
        except Exception:
            pass
        print(f"æœç´¢å¤±è´¥: {e}")
        return None

def _load_json(path: str, default):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return default


def _save_json(path: str, obj):
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(obj, f, ensure_ascii=False, indent=2)
    except Exception:
        pass


def _is_probable_tool_page(url: str, title: str, desc: str) -> bool:
    """Heuristic for tool-type pages.

    Plan B is noisy; prioritize "not spam" over "perfectly new".
    """
    t = (title or "").lower()
    d = (desc or "").lower()
    blob = t + " " + d

    # reject obvious listicles / SEO sludge
    bad = [
        "best ai tools",
        "top ai tools",
        "ultimate guide",
        "list of",
        "coupon",
        "discount",
        "affiliat",
    ]
    if any(b in blob for b in bad):
        return False

    # must at least look AI/dev related
    must = ["ai", "llm", "agent", "rag", "open source", "github", "model", "prompt", "inference", "copilot", "coding"]
    if not any(m in blob for m in must):
        return False

    return True


def search_tools():
    """åŠ¨æ€æŠ“å·¥å…·æ¨èï¼ˆæ–¹æ¡ˆB / å®ç¼ºæ¯‹æ»¥ï¼‰ã€‚

    ç›®æ ‡ï¼šå®å¯åªæœ‰ 1-2 ä¸ªï¼Œä¹Ÿä¸å¡â€œè®ºå›å¸–å­/åˆé›†é¡µ/ä¸‹è½½ç«™/æ•™ç¨‹ç›˜ç‚¹â€ã€‚
    """

    BRAVE_API_KEY = os.environ.get('BRAVE_API_KEY')
    if not BRAVE_API_KEY:
        return None

    history_path = os.path.join(REPO_DIR, "tools_history.json")
    history = _load_json(history_path, {"recent": []})
    recent = set(history.get("recent", [])[-80:])

    # Query set: bias toward real tool artifacts (repo/release/package) + official posts.
    queries = [
        "site:github.com (release OR released) (AI OR LLM OR agent OR RAG) open source",
        "site:github.com (v0. OR v1. OR v2.) (AI OR LLM OR agent) release notes",
        "site:pypi.org AI tool released",
        "site:npmjs.com AI agent framework",
        "site:huggingface.co/spaces new AI tool",
        "new open source AI tool released GitHub",
    ]

    deny_hosts = {
        # social / forums / noisy
        "reddit.com", "www.reddit.com",
        "news.ycombinator.com",
        "medium.com",
        "substack.com",
        "dev.to",
        "discuss.huggingface.co",
        # download farms / aggregators
        "uptodown.com", "ollama.en.uptodown.com",
        # generic social
        "pinterest.com", "facebook.com", "twitter.com", "x.com", "instagram.com", "tiktok.com",
    }

    def is_bad_tool_page(url_i: str, title: str, desc: str) -> bool:
        p = urlparse(url_i)
        host = p.netloc.lower().replace("www.", "")
        path = (p.path or "").lower()
        blob = (title + " " + desc).lower()

        # listicles / roundups / guides
        bad_words = ["best ", "top ", "ultimate", "definitive", "guide", "list of", "alternatives", "reviews", "pricing"]
        if any(w in blob for w in bad_words):
            return True

        # forums / threads
        if any(x in path for x in ["/forum", "/forums", "/thread", "/threads", "/discussion", "/discuss"]):
            return True

        # producthunt category/review/alternatives etc.
        if host == "producthunt.com" and ("/categories/" in path or "/reviews" in path or "/alternatives" in path):
            return True

        return False

    def looks_like_tool_artifact(url_i: str) -> bool:
        p = urlparse(url_i)
        host = p.netloc.lower().replace("www.", "")
        path = (p.path or "")
        if host == "github.com":
            parts = [x for x in path.split("/") if x]
            if len(parts) < 2:
                return False
            # reject issues / pulls / discussions (not a tool artifact)
            if len(parts) >= 3 and parts[2] in {"issues", "pull", "pulls", "discussions"}:
                return False
            return True
        if host == "pypi.org":
            return path.startswith("/project/")
        if host == "npmjs.com":
            return path.startswith("/package/")
        if host == "huggingface.co":
            return path.startswith("/spaces/") or path.startswith("/models/")
        return True

    picked = []
    seen = set()

    for i, q in enumerate(queries):
        if len(picked) >= 3:
            break
        if i > 0:
            time.sleep(1.2)  # Brave free plan: 1 QPS

        params = {"q": q, "count": 20, "freshness": "pw"}
        url = "https://api.search.brave.com/res/v1/web/search?" + urllib.parse.urlencode(params)
        req = urllib.request.Request(url, headers={
            'Accept': 'application/json',
            'X-Subscription-Token': BRAVE_API_KEY
        })

        try:
            with urllib.request.urlopen(req, timeout=30) as response:
                data = json.loads(response.read().decode('utf-8'))
        except Exception:
            continue

        for item in (data.get('web', {}) or {}).get('results', []):
            title = clean_text(item.get('title', ''))
            url_i = item.get('url', '')
            desc = clean_text(item.get('description', ''))

            if not title or not url_i:
                continue

            host = (item.get('meta_url') or {}).get('netloc') or urlparse(url_i).netloc
            host = (host or "").lower().replace("www.", "")

            if host in deny_hosts:
                continue
            if url_i in recent:
                continue
            if _is_probable_homepage_or_section(url_i, title):
                continue
            if is_bad_tool_page(url_i, title, desc):
                continue
            if not looks_like_tool_artifact(url_i):
                continue

            # cheap de-dupe
            key = (re.sub(r"\W+", "", title.lower())[:90], host)
            if key in seen:
                continue
            seen.add(key)

            # date from page_age when possible
            date = None
            dt = _parse_iso_dt(item.get("page_age"))
            if dt:
                date = dt.strftime("%Y-%m-%d")

            picked.append({
                "name": title,
                "url": url_i,
                "desc": desc,
                "source": get_source_name(url_i),
                "date": date,
            })

            if len(picked) >= 3:
                break

    if picked:
        history.setdefault("recent", [])
        history["recent"].extend([p["url"] for p in picked])
        history["recent"] = history["recent"][-300:]
        _save_json(history_path, history)

    return picked


def generate_daily():
    """ç”Ÿæˆæ—¥æŠ¥"""
    data = search_news()
    
    md_file = os.path.join(REPO_DIR, 'daily', f'{TODAY}.md')
    
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(f"# AI Daily Â· {TODAY}\n\n")
        f.write(f"æ—¥æœŸ: {TODAY} {datetime.now().strftime('%H:%M')}\n\n")
        
        # ä»Šæ—¥æ–°é—»
        f.write("## ğŸ“° ä»Šæ—¥æ–°é—»\n\n")
        
        if data and 'web' in data:
            for item in data.get('web', {}).get('results', [])[:5]:
                title = clean_text(item.get('title', ''))
                url = item.get('url', '')
                desc = clean_text(item.get('description', ''))
                
                if title and url:
                    # ä½¿ç”¨DeepSeekç¿»è¯‘æ ‡é¢˜
                    title_cn = translate_with_deepseek(title)
                    
                    source = get_source_name(url)
                    
                    f.write(f"### {title_cn}\n\n")
                    f.write(f"æ¥æº: [{source}]({url})\n\n")
                    if desc:
                        # ä½¿ç”¨DeepSeekç¿»è¯‘æè¿°
                        desc_cn = translate_with_deepseek(desc)
                        f.write(f"{desc_cn}\n\n")
                    f.write(f"[é˜…è¯»åŸæ–‡]({url})\n\n")
                    f.write("---\n\n")
        
        # å·¥å…·æ¨èï¼ˆæ–¹æ¡ˆBï¼šåŠ¨æ€æŠ“æ–°å“/æ›´æ–°ï¼‰
        f.write("## ğŸ› ï¸ å·¥å…·æ¨è\n\n")

        tool_items = search_tools()
        if tool_items:
            for t in tool_items[:3]:
                name = t.get("name") or t.get("title") or "(æœªå‘½åå·¥å…·)"
                url = t.get("url") or ""
                desc = t.get("desc") or ""
                source = t.get("source") or get_source_name(url)
                date = t.get("date")

                name_cn = translate_with_deepseek(name)
                desc_cn = translate_with_deepseek(desc) if desc else ""

                f.write(f"### {name_cn}\n\n")
                if date:
                    f.write(f"æ¥æº: [{source}]({url})ï½œæ—¥æœŸ: {date}\n\n")
                else:
                    f.write(f"æ¥æº: [{source}]({url})\n\n")
                if desc_cn:
                    f.write(f"{desc_cn}\n\n")
                f.write(f"[è®¿é—®]({url})\n\n")
                f.write("---\n\n")
        else:
            # fallback: still avoid total empty section
            f.write("ä»Šå¤©æ²¡æŠ“åˆ°è¶³å¤Ÿé è°±çš„æ–°å·¥å…·æ›´æ–°ï¼ˆå¯èƒ½è¢«é™æµ/æ¥æºä¸ç¨³å®šï¼‰ã€‚\n\n")
            f.write("å»ºè®®ï¼šæ˜å¤©å†çœ‹ï¼Œæˆ–æˆ‘å¯ä»¥æ”¹æˆâ€˜å·¥å…·æ± è½®æ¢â€™ä¿è¯æ¯å¤©éƒ½æœ‰ã€‚\n\n")
            f.write("---\n\n")
        
        # å½’æ¡£
        f.write("## ğŸ“š å½’æ¡£\n")
        f.write(f"- [{TODAY}](./{TODAY}.html)\n")
    
    print(f"âœ“ åˆ›å»ºæ—¥æŠ¥: {md_file}")
    
    # æ›´æ–°README
    readme_file = os.path.join(REPO_DIR, 'README.md')
    if os.path.exists(readme_file):
        with open(readme_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ç§»é™¤æ—§æ¡ç›®ï¼Œæ·»åŠ æ–°æ¡ç›®
        content = re.sub(r'- \[{}\].*\n'.format(TODAY), '', content)
        content = re.sub(r'(\n## ğŸ“š å½’æ¡£)', f'\n- [{TODAY}](./daily/{TODAY}.md)\n\\1', content)
        
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write(content)
        print("âœ“ æ›´æ–°README")
    
    return md_file

def generate_html():
    """ç”ŸæˆHTML"""
    print("ğŸ”„ ç”ŸæˆHTMLé¡µé¢...")
    convert_script = os.path.join(REPO_DIR, 'convert.py')
    result = subprocess.run(['python3', convert_script], capture_output=True, text=True)
    if result.returncode == 0:
        print("âœ“ ç”ŸæˆHTMLé¡µé¢")
    else:
        print(f"âœ— HTMLç”Ÿæˆå¤±è´¥: {result.stderr}")

def commit_and_push():
    """æäº¤å¹¶æ¨é€"""
    print("ğŸ“¤ æ¨é€åˆ°GitHub...")
    
    # è®¾ç½®remote
    if GITHUB_TOKEN:
        remote_url = f"https://{GITHUB_TOKEN}@github.com/yunhongfeng-tracy/ai-daily.git"
        subprocess.run(['git', 'remote', 'set-url', 'origin', remote_url], cwd=REPO_DIR, capture_output=True)
    
    # é…ç½®git
    subprocess.run(['git', 'config', 'user.name', 'tracy-bot'], cwd=REPO_DIR, capture_output=True)
    subprocess.run(['git', 'config', 'user.email', 'bot@tracy.ai'], cwd=REPO_DIR, capture_output=True)
    
    # æ·»åŠ å¹¶æäº¤
    subprocess.run(['git', 'add', '-A'], cwd=REPO_DIR, capture_output=True)
    result = subprocess.run(['git', 'status', '--porcelain'], cwd=REPO_DIR, capture_output=True, text=True)
    
    if result.stdout.strip():
        subprocess.run(['git', 'commit', '-m', f'AI Daily: {TODAY}'], cwd=REPO_DIR, capture_output=True)
        if GITHUB_TOKEN:
            subprocess.run(['git', 'push', 'origin', 'main'], cwd=REPO_DIR, capture_output=True)
            print("âœ“ å·²æ¨é€åˆ°GitHub")
    else:
        print("âœ“ æ— æ–°å†…å®¹ï¼Œè·³è¿‡æäº¤")

def main():
    print("=" * 40)
    md_file = generate_daily()
    generate_html()
    commit_and_push()
    print("=" * 40)
    print(f"ğŸ‰ AIæ—¥æŠ¥ç”Ÿæˆå®Œæˆï¼")
    print(f"ğŸ“… æ—¥æœŸ: {TODAY}")

if __name__ == '__main__':
    main()
