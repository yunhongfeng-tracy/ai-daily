#!/usr/bin/env python3
"""
AI Daily Generator - è‡ªåŠ¨ç”Ÿæˆæ¯æ—¥AIæ–°é—»å’Œå·¥å…·æ¨è
"""

import os
import re
import json
import subprocess
import urllib.request
from datetime import datetime
from urllib.parse import urlparse

# é…ç½®
REPO_DIR = "/root/.openclaw/workspace/ai-daily"
TODAY = datetime.now().strftime('%Y-%m-%d')
NOW = datetime.now().strftime('%Y-%m-%d %H:%M')
DEEPSEEK_API_KEY = os.environ.get('DEEPSEEK_API_KEY', 'sk-7bc8f2dcf1734756bd81c55af2413f80')
GITHUB_TOKEN = os.environ.get('GITHUB_TOKEN')

# DeepSeekç¿»è¯‘å‡½æ•°
def translate_with_deepseek(text):
    """ä½¿ç”¨DeepSeek APIç¿»è¯‘ä¸ºä¸­æ–‡"""
    if not text or len(text.strip()) < 5:
        return text
    
    # ç®€å•æœ¯è¯­ç›´æ¥æŸ¥è¯å…¸ï¼ˆå¿«é€Ÿï¼‰
    simple_trans = {
        'AI': 'äººå·¥æ™ºèƒ½',
        'Artificial Intelligence': 'äººå·¥æ™ºèƒ½',
        'Machine Learning': 'æœºå™¨å­¦ä¹ ',
        'Deep Learning': 'æ·±åº¦å­¦ä¹ ',
        'LLM': 'å¤§è¯­è¨€æ¨¡å‹',
        'OpenAI': 'OpenAI',
        'Anthropic': 'Anthropic',
        'Google': 'è°·æ­Œ',
        'Microsoft': 'å¾®è½¯',
        'Reuters': 'è·¯é€ç¤¾',
        'BBC': 'BBC',
        'MIT': 'éº»çœç†å·¥',
        'TechCrunch': 'TechCrunch',
        'NVIDIA': 'è‹±ä¼Ÿè¾¾',
        'Meta': 'Meta',
        'Amazon': 'äºšé©¬é€Š',
        'Apple': 'è‹¹æœ',
    }
    
    # å…ˆåšç®€å•æ›¿æ¢
    result = text
    for eng, chi in simple_trans.items():
        result = re.sub(r'\b' + re.escape(eng) + r'\b', chi, result, flags=re.IGNORECASE)
    
    # å¦‚æœåŒ…å«å¤æ‚å¥å­ï¼Œç”¨DeepSeekç¿»è¯‘
    if len(text) > 30 and not text.startswith('http'):
        try:
            url = "https://api.deepseek.com/chat/completions"
            payload = {
                "model": "deepseek-chat",
                "messages": [
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIç§‘æŠ€æ–°é—»ç¿»è¯‘ã€‚è¯·å°†è‹±æ–‡ç¿»è¯‘æˆç®€æ´çš„ä¸­æ–‡ï¼Œä¿ç•™ä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§ã€‚åªéœ€è¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"
                    },
                    {
                        "role": "user",
                        "content": f"ç¿»è¯‘è¿™æ®µè‹±æ–‡æ–°é—»æ ‡é¢˜å’Œæ‘˜è¦ï¼š\n\n{text}"
                    }
                ],
                "max_tokens": 500,
                "temperature": 0.3
            }
            
            data = json.dumps(payload).encode('utf-8')
            req = urllib.request.Request(url, data=data, method='POST')
            req.add_header('Content-Type', 'application/json')
            req.add_header('Authorization', f'Bearer {DEEPSEEK_API_KEY}')
            
            with urllib.request.urlopen(req, timeout=30) as response:
                result_data = json.loads(response.read().decode('utf-8'))
                translated = result_data['choices'][0]['message']['content'].strip()
                # æ¸…ç†å¯èƒ½çš„å¼•å·
                translated = re.sub(r'^["\']|["\']$', '', translated)
                return translated
        except Exception as e:
            print(f"  ç¿»è¯‘APIè°ƒç”¨å¤±è´¥: {e}")
            return result
    
    return result

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬"""
    if not text:
        return ''
    # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    # æ›¿æ¢HTMLå®ä½“
    text = text.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
    text = text.replace('&#x27;', "'").replace('&#39;', "'").replace('&quot;', '"')
    text = text.replace('&ldquo;', '"').replace('&rdquo;', '"').replace('&lsquo;', "'").replace('&rsquo;', "'")
    # ç§»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
    text = re.sub(r'\s+', ' ', text).strip()
    # ç§»é™¤æ¨å¹¿ä¿¡æ¯
    text = re.sub(r'Subscribe.*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'Register.*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'Login.*', '', text, flags=re.IGNORECASE)
    return text.strip()

def get_source_name(url):
    """ä»URLæå–æ¥æº"""
    if not url:
        return 'æœªçŸ¥æ¥æº'
    try:
        parsed = urlparse(url)
        source = parsed.netloc.replace('www.', '').split('/')[0]
        # å¸¸è§æ¥æºæ˜ å°„
        source_map = {
            'reuters.com': 'è·¯é€ç¤¾',
            'bbc.com': 'BBC',
            'techcrunch.com': 'TechCrunch',
            'mit.edu': 'éº»çœç†å·¥',
            'theverge.com': 'The Verge',
            'wired.com': 'Wired',
            'artificialintelligence-news.com': 'AIæ–°é—»',
        }
        for eng, chi in source_map.items():
            if eng in source:
                return chi
        return source.split('.')[0].title()
    except:
        return 'æœªçŸ¥æ¥æº'

def search_news():
    """æœç´¢AIæ–°é—»"""
    print(f"ğŸ¤– AI Daily Generator - {TODAY}")
    print("ğŸ“° æœç´¢AIæ–°é—»...")
    
    BRAVE_API_KEY = os.environ.get('BRAVE_API_KEY', 'BSABJykguZY7fMv9-C0etQUd4zEs1Yt')
    url = f"https://api.search.brave.com/res/v1/web/search?q=AI+artificial+intelligence+news+today&count=8"
    req = urllib.request.Request(url, headers={
        'Accept': 'application/json',
        'X-Subscription-Token': BRAVE_API_KEY
    })
    
    try:
        with urllib.request.urlopen(req, timeout=30) as response:
            data = response.read().decode('utf-8')
            return json.loads(data)
    except Exception as e:
        print(f"æœç´¢å¤±è´¥: {e}")
        return None

def generate_daily():
    """ç”Ÿæˆæ—¥æŠ¥"""
    data = search_news()
    
    md_file = os.path.join(REPO_DIR, 'daily', f'{TODAY}.md')
    
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(f"# AI Daily Â· {TODAY}\n\n")
        f.write(f"æ—¥æœŸ: {TODAY} {datetime.now().strftime('%H:%M')}\n\n")
        
        # ä»Šæ—¥æ–°é—»
        f.write("## ğŸ“° ä»Šæ—¥æ–°é—»\n\n")
        
        if data and 'web' in data:
            for item in data.get('web', {}).get('results', [])[:5]:
                title = clean_text(item.get('title', ''))
                url = item.get('url', '')
                desc = clean_text(item.get('description', ''))
                
                if title and url:
                    # ä½¿ç”¨DeepSeekç¿»è¯‘æ ‡é¢˜
                    title_cn = translate_with_deepseek(title)
                    
                    source = get_source_name(url)
                    
                    f.write(f"### {title_cn}\n\n")
                    f.write(f"æ¥æº: [{source}]({url})\n\n")
                    if desc:
                        # ä½¿ç”¨DeepSeekç¿»è¯‘æè¿°
                        desc_cn = translate_with_deepseek(desc)
                        f.write(f"{desc_cn}\n\n")
                    f.write(f"[é˜…è¯»åŸæ–‡]({url})\n\n")
                    f.write("---\n\n")
        
        # å·¥å…·æ¨è
        f.write("## ğŸ› ï¸ å·¥å…·æ¨è\n\n")
        tools = [
            ("v0.dev - AI UIç”Ÿæˆå™¨", "ç”±Vercelæ¨å‡ºçš„AIç•Œé¢ç”Ÿæˆå™¨ï¼Œåªéœ€æè¿°éœ€æ±‚å³å¯è‡ªåŠ¨ç”ŸæˆReact/Tailwindç»„ä»¶ã€‚", "https://v0.app"),
            ("Cursor - AIä»£ç ç¼–è¾‘å™¨", "ä¸“ä¸ºAIè¾…åŠ©ç¼–ç¨‹è®¾è®¡çš„IDEï¼ŒåŸºäºVS Codeï¼Œæ”¯æŒæ™ºèƒ½ä»£ç è¡¥å…¨å’Œé‡æ„å»ºè®®ã€‚", "https://cursor.com"),
            ("Perplexity - AIæœç´¢å¼•æ“", "ç»“åˆå¤§è¯­è¨€æ¨¡å‹çš„æœç´¢å¼•æ“ï¼Œæä¾›å¸¦æœ‰å¼•ç”¨æ¥æºçš„ç­”æ¡ˆï¼Œæ”¯æŒå¤šç§è¯­è¨€ã€‚", "https://www.perplexity.ai"),
        ]
        
        for name, desc, link in tools:
            f.write(f"### {name}\n\n")
            f.write(f"ğŸ“ {desc}\n\n")
            f.write(f"ğŸ”— [è®¿é—®]({link})\n\n")
            f.write("---\n\n")
        
        # å½’æ¡£
        f.write("## ğŸ“š å½’æ¡£\n")
        f.write(f"- [{TODAY}](./{TODAY}.html)\n")
    
    print(f"âœ“ åˆ›å»ºæ—¥æŠ¥: {md_file}")
    
    # æ›´æ–°README
    readme_file = os.path.join(REPO_DIR, 'README.md')
    if os.path.exists(readme_file):
        with open(readme_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ç§»é™¤æ—§æ¡ç›®ï¼Œæ·»åŠ æ–°æ¡ç›®
        content = re.sub(r'- \[{}\].*\n'.format(TODAY), '', content)
        content = re.sub(r'(\n## ğŸ“š å½’æ¡£)', f'\n- [{TODAY}](./daily/{TODAY}.md)\n\\1', content)
        
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write(content)
        print("âœ“ æ›´æ–°README")
    
    return md_file

def generate_html():
    """ç”ŸæˆHTML"""
    print("ğŸ”„ ç”ŸæˆHTMLé¡µé¢...")
    convert_script = os.path.join(REPO_DIR, 'convert.py')
    result = subprocess.run(['python3', convert_script], capture_output=True, text=True)
    if result.returncode == 0:
        print("âœ“ ç”ŸæˆHTMLé¡µé¢")
    else:
        print(f"âœ— HTMLç”Ÿæˆå¤±è´¥: {result.stderr}")

def commit_and_push():
    """æäº¤å¹¶æ¨é€"""
    print("ğŸ“¤ æ¨é€åˆ°GitHub...")
    
    # è®¾ç½®remote
    if GITHUB_TOKEN:
        remote_url = f"https://{GITHUB_TOKEN}@github.com/yunhongfeng-tracy/ai-daily.git"
        subprocess.run(['git', 'remote', 'set-url', 'origin', remote_url], cwd=REPO_DIR, capture_output=True)
    
    # é…ç½®git
    subprocess.run(['git', 'config', 'user.name', 'tracy-bot'], cwd=REPO_DIR, capture_output=True)
    subprocess.run(['git', 'config', 'user.email', 'bot@tracy.ai'], cwd=REPO_DIR, capture_output=True)
    
    # æ·»åŠ å¹¶æäº¤
    subprocess.run(['git', 'add', '-A'], cwd=REPO_DIR, capture_output=True)
    result = subprocess.run(['git', 'status', '--porcelain'], cwd=REPO_DIR, capture_output=True, text=True)
    
    if result.stdout.strip():
        subprocess.run(['git', 'commit', '-m', f'AI Daily: {TODAY}'], cwd=REPO_DIR, capture_output=True)
        if GITHUB_TOKEN:
            subprocess.run(['git', 'push', 'origin', 'main'], cwd=REPO_DIR, capture_output=True)
            print("âœ“ å·²æ¨é€åˆ°GitHub")
    else:
        print("âœ“ æ— æ–°å†…å®¹ï¼Œè·³è¿‡æäº¤")

def main():
    print("=" * 40)
    md_file = generate_daily()
    generate_html()
    commit_and_push()
    print("=" * 40)
    print(f"ğŸ‰ AIæ—¥æŠ¥ç”Ÿæˆå®Œæˆï¼")
    print(f"ğŸ“… æ—¥æœŸ: {TODAY}")

if __name__ == '__main__':
    main()
